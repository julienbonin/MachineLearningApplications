{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuRNCQ13k9HXKeotnCG8+/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julienbonin/MachineLearningApplications/blob/master/Chapters/Chapter%205/Chapter5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSc-fvJ9sj4f"
      },
      "source": [
        "#Problem 1:\n",
        "###What is the fundamental idea behind Support Vector Machines?\n",
        "\n",
        "#Answer:\n",
        "###The fundamental idea of SVMs is to separate classes of data in a way that maximizes the distance between the two closest instances of the two classes. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRil1DN7teI-"
      },
      "source": [
        "#Problem 2:\n",
        "\n",
        "###What is a support vector?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###A suport vector is a datapoint(s) that is used to determine a decision boundary. From the explanaion to problem 1, SVMs want to maximize the distance between the two closest instances of the two classes. These two instances are the support vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ubEF394tjDu"
      },
      "source": [
        "#Problem 3:\n",
        "\n",
        "###Why is it important to scale the inputs when using SVMs?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###SVMs are sensitive to feature scales. If the scaling of one feature is much larger than another, the SVM may not find the largest possible space between the classes.\n",
        "\n",
        "####aside: This is evident from looking at Figure 5-2, however, I'm still not convinced of the reasoning behind this. For example, I don't understand why the unscaled features on the left of the figure show a nearly horizontal line. If the goal of SVM is to maximize the distance between the two closest instances of two classes, why wouldn't the unscalled data work just as the scaled data? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64q7500ntszX"
      },
      "source": [
        "#Problem 4:\n",
        "\n",
        "###Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###The SVM classifier can tell you if a new data instance falls somewhere in the middle of the 'road' or 'off road' which could be used to determing some type of confidence about the classification of the new datapoint. The svm cannot tell you a probability since it only determinse whether or not a new datapoint is on one side of the street or the other. It's decision boundary is either yes or no, instead of having a probability or one class vs another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-2gpWNOt0P-"
      },
      "source": [
        "#Problem 5:\n",
        "\n",
        "###Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###For this example, you should use the primal form of the SVM problem to train the model, since the dual probelm is only faster to solve when the number of training instances are smaller than the number of features (which is not the case here).\n",
        "\n",
        "####aside: The book seems to gloss over the details of primal and dual problems. I looked up some additional information and it seems like these concept are related to linear programming (or at least there's some representation of these ideas in LP)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt02-Nwnt6Yz"
      },
      "source": [
        "#Problem 6:\n",
        "\n",
        "###Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease γ (gamma)? What about C?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###From the equation for the RBF kernel, it seems like increasing the gamma will decrease K(a,b) and decreasing gamma will increase K(a,b). Increasing K should make our decision boundary wider (similar to increasing C). Since the model is underfitting the data, we should increase gamma and/or C in order to separate the instances of each class more than they are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqToh4REuCUi"
      },
      "source": [
        "#Problem 7:\n",
        "\n",
        "###How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###The difference between the hard and soft margin SVM classifiers is the additional C[SUM_1toM(ζ i)]. So if the number of parameters for the hard margin problem is np = n+1, then the soft margin problem with have n'p = n+1+m parameters.\n",
        "\n",
        "###Therefore, \n",
        "###H is a (n+m+1)x(n+m+1) matrix,\n",
        "###f is (n+m+1)x1 vector\n",
        "###A is the same as for hard margin SVMs\n",
        "###b is the same as for hard margin SVMs \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5iZREZ8ASVv"
      },
      "source": [
        "#Problem 8:\n",
        "\n",
        "###Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model.\n",
        "\n",
        "####note: The code below is directly from the solutions repo. For this problem (and problem 9), I felt it necessary to copy from the repo. I feel like the concepts related to SVMs aren't clicking for me. This isn't the first time I've learned about SVMs. About a year ago, I took an edx course on ML, and SVMs where part of the curriculum. I understand the big picture, but I think I may be getting bogged down in the details of how this method works because some of the concepts (like Kernelization) isn't intuitive to me."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39pf8VfI_52v"
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "setosa_or_versicolor = (y == 0) | (y == 1)\n",
        "X = X[setosa_or_versicolor]\n",
        "y = y[setosa_or_versicolor]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ggzSfZsA4P3",
        "outputId": "99bb8597-004d-4181-9f51-92bca20b2ed8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "C = 5\n",
        "alpha = 1 / (C * len(X))\n",
        "\n",
        "lin_clf = LinearSVC(loss=\"hinge\", C=C, random_state=42)\n",
        "svm_clf = SVC(kernel=\"linear\", C=C)\n",
        "sgd_clf = SGDClassifier(loss=\"hinge\", learning_rate=\"constant\", eta0=0.001, alpha=alpha,\n",
        "                        max_iter=1000, tol=1e-3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "lin_clf.fit(X_scaled, y)\n",
        "svm_clf.fit(X_scaled, y)\n",
        "sgd_clf.fit(X_scaled, y)\n",
        "\n",
        "print(\"LinearSVC:                   \", lin_clf.intercept_, lin_clf.coef_)\n",
        "print(\"SVC:                         \", svm_clf.intercept_, svm_clf.coef_)\n",
        "print(\"SGDClassifier(alpha={:.5f}):\".format(sgd_clf.alpha), sgd_clf.intercept_, sgd_clf.coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC:                    [0.28474272] [[1.05364736 1.09903308]]\n",
            "SVC:                          [0.31896852] [[1.1203284  1.02625193]]\n",
            "SGDClassifier(alpha=0.00200): [0.117] [[0.77714169 0.72981762]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMNGDQOIBMwB"
      },
      "source": [
        "# Compute the slope and bias of each decision boundary\n",
        "w1 = -lin_clf.coef_[0, 0]/lin_clf.coef_[0, 1]\n",
        "b1 = -lin_clf.intercept_[0]/lin_clf.coef_[0, 1]\n",
        "w2 = -svm_clf.coef_[0, 0]/svm_clf.coef_[0, 1]\n",
        "b2 = -svm_clf.intercept_[0]/svm_clf.coef_[0, 1]\n",
        "w3 = -sgd_clf.coef_[0, 0]/sgd_clf.coef_[0, 1]\n",
        "b3 = -sgd_clf.intercept_[0]/sgd_clf.coef_[0, 1]\n",
        "\n",
        "# Transform the decision boundary lines back to the original scale\n",
        "line1 = scaler.inverse_transform([[-10, -10 * w1 + b1], [10, 10 * w1 + b1]])\n",
        "line2 = scaler.inverse_transform([[-10, -10 * w2 + b2], [10, 10 * w2 + b2]])\n",
        "line3 = scaler.inverse_transform([[-10, -10 * w3 + b3], [10, 10 * w3 + b3]])\n",
        "\n",
        "# Plot all three decision boundaries\n",
        "plt.figure(figsize=(11, 4))\n",
        "plt.plot(line1[:, 0], line1[:, 1], \"k:\", label=\"LinearSVC\")\n",
        "plt.plot(line2[:, 0], line2[:, 1], \"b--\", linewidth=2, label=\"SVC\")\n",
        "plt.plot(line3[:, 0], line3[:, 1], \"r-\", label=\"SGDClassifier\")\n",
        "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\") # label=\"Iris versicolor\"\n",
        "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\") # label=\"Iris setosa\"\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"upper center\", fontsize=14)\n",
        "plt.axis([0, 5.5, 0, 2])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfL5iTy8BYCC"
      },
      "source": [
        "#Problem 9\n",
        "\n",
        "###Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach?\n",
        "\n",
        "####Note: The code below is directly from the solutions repo. I've added annotations to explain what is being performed, along with any lack of understanding I may (or may not) have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dLtpRAvBn-j"
      },
      "source": [
        "# This cell is just as before with the mnist dataset. The first 60k is the training data, the rest is the test data\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "\n",
        "X = mnist[\"data\"]\n",
        "y = mnist[\"target\"].astype(np.uint8)\n",
        "\n",
        "X_train = X[:60000]\n",
        "y_train = y[:60000]\n",
        "X_test = X[60000:]\n",
        "y_test = y[60000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pq0EErRB7Ao"
      },
      "source": [
        "# This cell creates and fits a Linear SVC model to the training data\n",
        "lin_clf = LinearSVC(random_state=42)\n",
        "lin_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WryebQVICKXb"
      },
      "source": [
        "# This cell calculates the accuracy of the model to the training data. \n",
        "# The repo (after running this cell) mentions that the 89.5% accuracy\n",
        "# is pretty bad. I'm a little confused about why this would be bad, \n",
        "# since we don't want to overfit on the training data. 95-100%\n",
        "# accuracy definitely seems like overfitting the training set, so \n",
        "# I'm not sure what I'm missing here.\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = lin_clf.predict(X_train)\n",
        "accuracy_score(y_train, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsgN88-TEDXK"
      },
      "source": [
        "# This cell scales the data. This alludes back to the about problem (#3) where\n",
        "# I mention about the purpose of scalling the data. Clearly from running these\n",
        "# cells, you can see that the SVC classifier performs better if the data is \n",
        "# scaled, however, I don't think I fully understand why this is the case.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))\n",
        "X_test_scaled = scaler.transform(X_test.astype(np.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ak7l9MGE_Mw"
      },
      "source": [
        "# This cell does the same as the previous few cells, except with the scaled data.\n",
        "lin_clf = LinearSVC(random_state=42)\n",
        "lin_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = lin_clf.predict(X_train_scaled)\n",
        "accuracy_score(y_train, y_pred)\n",
        "\n",
        "svm_clf = SVC(gamma=\"scale\")\n",
        "svm_clf.fit(X_train_scaled[:10000], y_train[:10000])\n",
        "\n",
        "y_pred = svm_clf.predict(X_train_scaled)\n",
        "accuracy_score(y_train, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlfmbV7NFP66"
      },
      "source": [
        "# This cell contains the remainder of the code from the solutions repo.\n",
        "# Here, the hyper parameters and best model are selected and tested.\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import reciprocal, uniform\n",
        "\n",
        "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
        "rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2, cv=3)\n",
        "rnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000])\n",
        "\n",
        "rnd_search_cv.best_estimator_\n",
        "\n",
        "rnd_search_cv.best_score_\n",
        "\n",
        "rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\n",
        "accuracy_score(y_train, y_pred)\n",
        "\n",
        "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkY6JLDPGC_E"
      },
      "source": [
        "#Problem 10:\n",
        "\n",
        "###Train an SVM regressor on the California housing dataset.\n",
        "\n",
        "####note: This problem seems straigt forward, so I did much of the implementation myself. I used the solution repo as a guide. Anything that was taken from the solution is labeled at the top of the cell along with any comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGThC692GIVZ"
      },
      "source": [
        "# This was copied from the repo. It's essentially the same as in chapter 2\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing[\"data\"]\n",
        "y = housing[\"target\"]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYwOtzJ-IsY-",
        "outputId": "868fa03b-4aa3-40d5-c0e8-c53ad03231e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now that we have the data (X,y)=(data, labels), and the test and training set, we can work on \n",
        "# selecting a model. Similar to problem 9, I wanted to try to create a model without scaling the data first.\n",
        "\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "lin_svr = LinearSVR(random_state=42)\n",
        "lin_svr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
              "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
              "          random_state=42, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNuiHWEOJX-a",
        "outputId": "8377e259-7b7d-4281-a235-ae981554bd62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# After fitting the training data, lets see how it performs\n",
        "# In the housing example, we used MSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "y_pred = lin_svr.predict(X_train)\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "print('MSE: ', mse)\n",
        "print('RMSE: ', np.sqrt(mse))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE:  1.8317654224357183\n",
            "RMSE:  1.3534272874579256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QThSqPJtKe5r",
        "outputId": "c95d0015-5524-4298-c954-adb6836d241b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Comparing these results from the solutions repo, it appears that the error is much larger on\n",
        "# the unscaled data. Below is the code from the solution repo for the scaled data error.\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "lin_svr = LinearSVR(random_state=42)\n",
        "lin_svr.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = lin_svr.predict(X_train)\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "print('MSE: ', mse)\n",
        "print('RMSE: ', np.sqrt(mse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE:  5571.747899808996\n",
            "RMSE:  74.64414176483642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NUoYGH4LjUI"
      },
      "source": [
        "####note: I'm not sure what my error here is. For the scaled data, the MSE and RMSE seem to be very far from what I expected. I will review the code and continue from here after I fix the issue.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJa0bi1aLOfw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}