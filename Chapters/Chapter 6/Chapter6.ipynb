{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtHUlehph7yJvvIJMNGzfn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julienbonin/MachineLearningApplications/blob/master/Chapters/Chapter%206/Chapter6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S93zJUW918f"
      },
      "source": [
        "#Problem 1:\n",
        "\n",
        "###What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 1 million instances?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "Since we're using a binary tree (i.e. every node has at most two children) we can use the formula for computing the total number of nodes n a depth d for a binary tree. n = 2^d, where n is the number of nodes (here it's given by 1 million), and d is the depth. Plugging in the number of nodes and using algebra, we can see log2(1 million) ~ 20, assuming we have a complete tree. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLtj5mkt99Vd"
      },
      "source": [
        "#Problem 3:\n",
        "\n",
        "###If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###Yes. By decreasing the max_depth of a decision tree, you are limiting it's degrees of freedom, which reduces the risk of overfitting the trainnign set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbzhT4HG99jI"
      },
      "source": [
        "#Problem 4:\n",
        "\n",
        "###If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###No, since decision trees don't require feature scaling or centering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkIv9ZK_9-Cw"
      },
      "source": [
        "#Problem 7:\n",
        "\n",
        "###Train and fine-tune a Decision Tree for the moons dataset.\n",
        "####a. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4).\n",
        "####b. Split it into a training set and a test set using train_test_split().\n",
        "####c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes.\n",
        "####d. Train it on the full training set using these hyperparameters, and measure your modelâ€™s performance on the test set. You should get roughly 85% to 87% accuracy.\n",
        "\n",
        "#Answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5OeSgGuLTdX"
      },
      "source": [
        "#a\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=10000, noise=0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEzTl1hNLgCH"
      },
      "source": [
        "#b\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px4lT1gJLmng",
        "outputId": "c0c30d3c-aa66-49e9-df5f-7a381fc95a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#c\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'max_leaf_nodes': list(range(2, 30)), 'min_samples_split': [2, 3, 4]}\n",
        "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\n",
        "\n",
        "grid_search_cv.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 84 candidates, totalling 252 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 252 out of 252 | elapsed:    2.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features=None,\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              presort='deprecated',\n",
              "                                              random_state=42,\n",
              "                                              splitter='best'),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
              "                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
              "                                            22, 23, 24, 25, 26, 27, 28, 29],\n",
              "                         'min_samples_split': [2, 3, 4]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6JkGLbKL_tC",
        "outputId": "304559e3-f6e5-4dc7-ee99-8207fea81674",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#d\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = grid_search_cv.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8615"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uFWGT27MY9O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}