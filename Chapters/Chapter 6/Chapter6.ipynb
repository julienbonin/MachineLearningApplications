{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1L1OIqWestUlCku4LYo/O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julienbonin/MachineLearningApplications/blob/master/Chapters/Chapter%206/Chapter6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S93zJUW918f"
      },
      "source": [
        "#Problem 1:\n",
        "\n",
        "###What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 1 million instances?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "Since we're using a binary tree (i.e. every node has at most two children) we can use the formula for computing the total number of nodes n a depth d for a binary tree. n = 2^d, where n is the number of nodes (here it's given by 1 million), and d is the depth. Plugging in the number of nodes and using algebra, we can see log2(1 million) ~ 20, assuming we have a complete tree. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLtj5mkt99Vd"
      },
      "source": [
        "#Problem 3:\n",
        "\n",
        "###If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###Yes. By decreasing the max_depth of a decision tree, you are limiting it's degrees of freedom, which reduces the risk of overfitting the trainnign set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbzhT4HG99jI"
      },
      "source": [
        "#Problem 4:\n",
        "\n",
        "###If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "###No, since decision trees don't require feature scaling or centering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkIv9ZK_9-Cw"
      },
      "source": [
        "#Problem 7:\n",
        "\n",
        "###Train and fine-tune a Decision Tree for the moons dataset.\n",
        "####a. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4).\n",
        "####b. Split it into a training set and a test set using train_test_split().\n",
        "####c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes.\n",
        "####d. Train it on the full training set using these hyperparameters, and measure your modelâ€™s performance on the test set. You should get roughly 85% to 87% accuracy.\n",
        "\n",
        "#Answer:"
      ]
    }
  ]
}