{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM64P2NYxIq/pvIUsKX6xI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julienbonin/MachineLearningApplications/blob/master/Chapters/Chapter%204/Chapter4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stCXwN2J5VzB"
      },
      "source": [
        "# Problem 1:\n",
        "### What Linear Regression training algorithm can you use if you have a training set with millions of features?\n",
        "\n",
        "\n",
        "# Answer:\n",
        "###Since we have millions of features, Batch GD would be computationally costly since it will calculate the gradient of each feature. It would be best to use Stochastic GD or Mini-batch GD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JElHxDQb6E9r"
      },
      "source": [
        "# Problem 2:\n",
        "###Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?\n",
        "\n",
        "# Answer:\n",
        "###Gradient Decent would suffer from features having different scales by taking much longer to converge. One option to solve this problem would be to standardize the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4E46H7w6Fti"
      },
      "source": [
        "#Problem 3:\n",
        "\n",
        "###Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
        "\n",
        "#Answer\n",
        "\n",
        "###3)\tSince the Logistic Regression model is convex, GD would not get stuck on a local minimum. (i.e. there are no local minimum)\n",
        "\n",
        "####A note about convex shapes:\n",
        "####In another course I'm taking, we’ll be going over geometric algorithms later this semester. Reading ahead, I learned about an algorithm to generate a convex hull around sets of data (I assumed this could be useful for classification purposes). Recalling the question I previously asked in class regarding linear programming; those techniques also required convex areas. From what I've learned, convex geometry is any shape where you can draw a line from one point of the shape to any other point on/in the shape, and never go outside of the shape. Visualizing a convex curve (at least in two dimensions), it's easy to see how there could be no local minimums.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRC0JkFN6ZAc"
      },
      "source": [
        "#Problem 4:\n",
        "\n",
        "###Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?\n",
        "\n",
        "#Answer:\n",
        "###No, they do not all produce the same model. Mini-batch GD and Stochastic GD will bounce around the optimal solution. They are not using every feature like in GD, so every iteration they may use different features that will point to a different minimum direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E7F-v7h6ZNv"
      },
      "source": [
        "#Problem 5:\n",
        "\n",
        "###Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
        "\n",
        "#Answer:\n",
        "###If the validation error is consistently going up, the model is over fitting the data. You can fix this by using 'early stopping'. You want batch GD to stop when the validation error starts to increase because this means you have passed the minimum. Since batch GD uses the full datasets parameters at each iteration and the MSE curve is convex, you don't (shouldn't) have to worry about the possibility of the current iteration somehow being higher than the previous (like with stochastic or mini-batch GD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjQqpG_F6Za8"
      },
      "source": [
        "#Problem 6:\n",
        "\n",
        "###Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
        "\n",
        "#Answer:\n",
        "###No since there could be an instance where the validation error for the current iteration could be higher than the previous, but the next iteration may be lower. This is because it's only using some of the features rather than the entire training set. The idea behind mini-batch GD is to decrease the validation error on 'average'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sAOn2iF6Zof"
      },
      "source": [
        "#Problem 7:\n",
        "\n",
        "###Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
        "\n",
        "#Answer:\n",
        "###Either Mini-Batch GD or Stochastic GD will reach the vicinity of the optimal solution the fastest, but will continue bouncing around, never actually achieving the optimal solution. Batch GD will eventually achieve the optimal solution since it is decreasing its error on the entire training set. You can make Mini-Batch GD and Stochastic GD converge by decreasing the learning rate incrementally with each step.\n",
        "####Aside:\tI may be wrong, but analytically (i.e. dealing with infinite values) would Mini-Batch GD and Stochastic GD ever really reach an optimal solution? Clearly in a computing system they would since a computer can only represent a finite level of precision, and after each optimization technique gets close enough to an optimal value the error would not change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9X4WOYm6Z1K"
      },
      "source": [
        "#Problem 8:\n",
        "\n",
        "###Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
        "\n",
        "#Answer:\n",
        "###You are over-fitting the model to the training data. One solution is to increase the amount of Data points you're training the system on. Another is to develop a better model. A third is to regularize the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkXVYtQP6aAp"
      },
      "source": [
        "#Problem 9:\n",
        "\n",
        "###Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter α or reduce it?\n",
        "\n",
        "#Answer\n",
        "###If the training and validation error is high, then the model if probably underfitting the data (if the model was overfitting, it would perform well on the training set). This would result in lower variance, but higher bias. The regularization parameter α should be decreased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvgGUAJM6aNK"
      },
      "source": [
        "#Problem 10:\n",
        "\n",
        "###Why would you want to use:\n",
        "####• Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?\n",
        "####• Lasso instead of Ridge Regression?\n",
        "####• Elastic Net instead of Lasso?\n",
        "\n",
        "#Answer:\n",
        "###You would want to use Ridge regression over liner regression if you don't want your model to be too sporadic (for example, if there is a lot of variance in your data). You would want to use LASSO over Ridge regression if you want your model to be less affected by outliers in the data.  You would want to user Elastic Net over LASSO regression because LASSO may behave erratically in some circumstances. Also, Elastic Net regression can give you more control over balancing Ridge and LASSO regression since it is a mix of both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okley7XS6apU"
      },
      "source": [
        "#Problem 11:\n",
        "\n",
        "###Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?\n",
        "\n",
        "#Answer:\n",
        "###Since the SoftMax is a classifier that identifies separates data into certain groups, it would not be suitable for this task since day/night and inside/outside are not completely independent. It would be better to use two logistic regression classifiers, one for day/night and one for inside/outside."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45LmfdGobIyt"
      },
      "source": [
        "# Problem 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdVnni-UbgRg"
      },
      "source": [
        "# This is similar to the code for stochastic GD on page 127, with some minor adjustments.\n",
        "\n",
        "n_epochs = 50\n",
        "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
        "def learning_schedule(t):\n",
        "  return t0 / (t + t1)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "# theta = np.random.randn(2,1) # random initialization\n",
        "\n",
        "\n",
        "\n",
        "# X_b and y are the calculated and actual values of the training set respectively.\n",
        "for epoch in range(n_epochs):\n",
        "  for i in range(m):\n",
        "    # note: I removed the three lines below since for Batch GD, we use\n",
        "    #       the entire data set, not a randomly selected data point.\n",
        "    # random_index = np.random.randint(m) #\n",
        "    # xi = X_b[random_index:random_index+1]\n",
        "    # yi = y[random_index:random_index+1]\n",
        "    # gradients = 2 * X.T.dot(X.dot(theta) - y) # Calculate gradients for all data points\n",
        "\n",
        "    softmaxScores = softmax_score(X, theta)\n",
        "    p_hat = instance_probability(softmaxScores)\n",
        "\n",
        "    gradients = np.sum(np.multiply((p_hat-y),X)) # Cross entropy gradient vector [del_J] --> np.multiply() is elementwise\n",
        "    eta = learning_schedule(epoch * m + i)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "    # predict scores for the new theta\n",
        "    y_pred = np.argmax(X.dot(theta))\n",
        "\n",
        "    val_error = MSE(y, y_pred)\n",
        "\n",
        "    # The code below is from the book with some minor modifications\n",
        "    if val_error < minimum_val_error:\n",
        "      minimum_val_error = val_error\n",
        "      best_epoch = epoch\n",
        "      best_model = clone(theta)\n",
        "\n",
        "\n",
        "# Parameters: \n",
        "#             x = instance variable\n",
        "#             THETA = parameter matrix where each column (theta_k) is the parameter vector of each class\n",
        "def softmax_score(x, THETA):\n",
        "  return x.dot(THETA) # Returns an array of k scores for the instance variable x\n",
        "\n",
        "\n",
        "# P_hat from text on page \n",
        "def instance_probability(scores):\n",
        "  return np.exp(scores) / np.sum(np.exp(scores))  # Return an array of k probabilities for the instance variable x\n",
        "\n",
        "# Parameters:\n",
        "#             y = validation labels [actual instance classication] (vector)\n",
        "#             y_pred = predicted labels on validation set (vector)\n",
        "def MSE(y, y_pred):\n",
        "  return np.sum(np.square(y_pred - y)) / len(y)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}